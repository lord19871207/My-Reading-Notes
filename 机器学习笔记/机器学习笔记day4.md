
多变量线性回归
Python 代码：

def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))
    
len(X)   size(X)   shape(X)
的区别。  len返回行数和列数 中较大的那一个 ， size 返回元素个数 shape 返回行列数（x,y）


一般表达式中 常用 小写字母表示向量，大写字母表示矩阵

矩阵加法
同纬度各元素相加
不同纬度矩阵无法相加

数和矩阵的乘法
矩阵中每个元素执行乘法

矩阵与向量的乘法  
结果为向量
m行n列矩阵 乘以n行一列矩阵  结果为m行一列矩阵


矩阵与矩阵相乘




多个变量的 线性回归
可以将多个变量转化成 变量矩阵A
多个特征 转化成 特征 矩阵  B
A乘以B的转置 就打得到了 整体表达式


梯度下降 对于多元线性回归的处理

代价函数

代价函数的偏导数

当某些特征范围特别大 或特别小时
我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。
以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。
解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间
特征函数 变量缩放 。让他们的取值范围 更接近，可以避免 梯度下降时出来来回震荡 导致收敛缓慢


学习率
如何判断已经收敛
如何选择学习率的数值

可以自己修改参数模型，比如改成二次函数或者三次函数
修改模型后更需要注意 变量缩放


正规方程
直接求出最值
n的3次方 复杂度